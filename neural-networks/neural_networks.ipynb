{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WPROWADZENIE DO SZTUCZNEJ INTELIGENCJI - LABORATORIUM 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ AUTORZY: **MATEUSZ SZCZEPANOWSKI** i **ŁUKASZ STANISZEWSKI**\n",
    "+ NUMERY INDEKSÓW: **298988** i **304098**\n",
    "+ ADRESY E-MAIL: mateusz.szczepanowski.stud@pw.edu.pl i lukasz.staniszewski.stud@pw.edu.pl\n",
    "+ KIERUNEK: **INFORMATYKA**\n",
    "+ PRZEDMIOT: **Wprowadzenie do sztucznej inteligencji**\n",
    "+ ZADANIE: **[LINK](https://apps.usos.pw.edu.pl/apps/f/mBHyr3Rd/lab6.pdf)**\n",
    "+ SYSTEM OPERACYJNY: **Windows 10**\n",
    "+ JĘZYK PROGRAMOWANIA: **Python 3.8**\n",
    "+ TEMAT: **Należy zaimplementować perceptron wielowarstowowy oraz metodę uczącą go przy pomocy algorytmu propagacji wstecznej.Wykorzystując napisaną sieć neuronową należy wytrenować klasyfikator na\n",
    "zbiorze danych MNIST i zbadać jego działanie.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import niezbędnych modułów\n",
    "+ **keras.datasets.mnist** - w celu załadowania zbioru mnist\n",
    "+ **numpy** - do operacji na macierzach\n",
    "+ **math** - operacje matematyczne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ZBIÓR MNIST\n",
    "+ Zbiór MNIST jest zbiorem 60000 obrazów uczących i 10000 testujących o rozmiarze 28x28 zawierających odręcznie zapisane cyfry 0-9 w skali szarości.\n",
    "+ W celu operacji na zbiorze MNIST powstała funkcja GENERATE_MNIST_DATASETS() zwracająca próbki danych podzielonych na zbiór trenujący (50000), walidacyjny (10000) oraz testowy (10000). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mnist_datasets(is_validation_needed=True, how_many_train=50000):\n",
    "    # getting sets of data and reshaping them\n",
    "    (X_trainvalid, Y_trainvalid), (X_test, Y_test) = mnist.load_data()\n",
    "    # to get inputs as (754,1) length vector\n",
    "    X_trainvalid = X_trainvalid.reshape(X_trainvalid.shape[0], 1, 28*28)\n",
    "    X_trainvalid = X_trainvalid.astype('float32')\n",
    "    # div to get (0-1)\n",
    "    X_trainvalid /= 255\n",
    "    # same for test\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, 28*28)\n",
    "    X_test = X_test.astype('float32')\n",
    "    X_test /= 255\n",
    "    # if we want also validation_set\n",
    "    if is_validation_needed:\n",
    "        X_train = X_trainvalid[:how_many_train]\n",
    "        Y_train = Y_trainvalid[:how_many_train]\n",
    "        X_valid = X_trainvalid[how_many_train:]\n",
    "        Y_valid = Y_trainvalid[how_many_train:]\n",
    "        return (X_train, Y_train), (X_valid, Y_valid), (X_test, Y_test)\n",
    "    else:\n",
    "        return (X_trainvalid, Y_trainvalid), (X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Zdefiniowanie problemu\n",
    "### 3.1. Ogólnie\n",
    "+ Zadanie polega na zaimplementowaniu perceptronu wielowarstwowego, zostanie on zaimplementowany w formie klasy Network, przyjmującej w konstruktorze listę liczb odpowiadającą ilościom neuronów w poszczególnych wartstwach perceptronu.\n",
    "\n",
    "+ Dla klasy tej zostanie również zaimplementowana funkcja TRAIN_NETWORK() przyjmująca jako parametr zbiór trenujący model, a także parametr beta oraz minibatch_size (używany do stochastycznego spadku gradientu). Funkcja ta wykorzysta metodę spadku gradientu, do obliczenia gradientu wektora na podstawie gradientów podzbioru i użyje go w celu wykonania kroku GD. Pojedynczy gradient będzie liczony z wykorzystaniem wstecznej propagacji.\n",
    "\n",
    "+ Na końcu nastąpi walidacja hiperparametru (liczby neuronów ukrytych w modelu) i wytrenowanie go na najlepszym możliwie hiperparametrze.\n",
    "\n",
    "+ Na końcu zostanie przeprowadzone sprawdzenie działania sieci na zbiorze testowym."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Klasa Network\n",
    "+ Klasa network jest obiektem reprezentującym sieć neuronową, w konstruktorze podawane są parametry sieci w postaci listy, gdzie kolejne liczby w liście to ilości neuronów w poszczególnych warstwach.\n",
    "    + np. perceptron o warstwie wejściowej składającej się ze 100 wejść, 5 wyjść i 3 warstw ukrytych o  wielkościach 50, 30 i 10 otrzymuje na wejśćiu listę $ [100, 50, 30, 10, 5] $\n",
    "+ Klasa posiada takie pola jak:\n",
    "    + wielkości wartstw (layers_sizes)\n",
    "    + liczba warstw (number_of_layers)\n",
    "    + listę wektorów bias-ów neuronów dla poszczególnych warstw (biases)\n",
    "    + listę macierzy wag neuronów dla poszczególnych warstw (weights)\n",
    "    + liczbę klas do sklasyfikowania przez sieć (n_of_classess)\n",
    "+ Metody klasy:\n",
    "    + obliczanie wektoru klasyfikacji (forward_propagation)\n",
    "    + wykonanie wstecznej propagacji na otrzymanej próbce (backward_propagation)\n",
    "    + metoda gradientu prostego do optymalizacji (train_network)\n",
    "    + metoda oceniająca sieć na otrzymanym zbiorze testowym (score_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, layers_sizes):\n",
    "        self.layers_sizes = layers_sizes\n",
    "        self.number_of_layers = len(layers_sizes)\n",
    "        # initialization biases of neurons are from normal distribution\n",
    "        self.biases = [np.random.randn(y, 1) for y in layers_sizes[1:]]\n",
    "        # initialization weights of neurons are ~U(-1/sqrt(dim(input)),1/sqrt(dim(input)))\n",
    "        self.weights = [np.random.uniform(low=-1 / math.sqrt(layers_sizes[0]), high=1 / math.sqrt(layers_sizes[0]), size=(y, x)) for x, y in zip(layers_sizes[:-1], layers_sizes[1:])]\n",
    "        self.n_of_classess = layers_sizes[-1]\n",
    "        \n",
    "    def forward_propagation(self, z):\n",
    "        # returned z is array with indexes of classes and values between 0 and 1 (max will be chosen) and index is class\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            if w.shape[1] != z.shape[0]:\n",
    "                z = z.transpose()\n",
    "            z = sigmoid(np.dot(w, z) + b)\n",
    "        return z\n",
    "\n",
    "    def backward_propagation(self, vector_input, values):\n",
    "        # first we need matrixes of neuron's weights and biases to be zeros\n",
    "        grad_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        grad_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # input should be vertical vector\n",
    "        vector_input = vector_input.transpose()\n",
    "        # first activation layer is vector of inputs\n",
    "        activations = [vector_input]\n",
    "        # current_activation\n",
    "        curr_activation = vector_input\n",
    "        # generating list of layers of z's for neurons\n",
    "        z_list = []\n",
    "        # for each layer of neurons\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            # getting list of z's for layer of neurons\n",
    "            z = np.dot(w, curr_activation) + b\n",
    "            # appending it to list\n",
    "            z_list.append(z)\n",
    "            # new activation is sigmoid(before_activation)\n",
    "            curr_activation = sigmoid(z)\n",
    "            # and adding as new layer of activation\n",
    "            activations.append(curr_activation)\n",
    "        # important case: different complex derivatives for last layer (output layer) \n",
    "        # ∂C/∂zL = ∂C/∂aL x ∂aL/∂zL\n",
    "        deriv_cost_z = 2 * (activations[-1] - values) * sigmoid_derivative(z_list[-1])\n",
    "        # ∂C/∂bL = 1 x ∂C/∂zL\n",
    "        grad_b[-1] = deriv_cost_z\n",
    "        # ∂C/∂wL = ∂C/∂zL x aL-1\n",
    "        grad_w[-1] = np.dot(deriv_cost_z, activations[-2].transpose())\n",
    "        # going backward from second-last layer to first\n",
    "        for layer in range(2, self.number_of_layers):\n",
    "            curr_z = z_list[-layer]\n",
    "            # ∂C/∂zK = ∂zK+1/∂aK x ∂C/∂zK+1 x ∂aK/∂zK\n",
    "            deriv_cost_z = np.dot(self.weights[-layer + 1].transpose(), deriv_cost_z) * sigmoid_derivative(curr_z)\n",
    "            # ∂C/∂bK = 1 x ∂C/∂zK\n",
    "            grad_b[-layer] = deriv_cost_z\n",
    "            # ∂C/∂wL = ∂C/∂zK x aK-1\n",
    "            grad_w[-layer] = np.dot(deriv_cost_z, activations[-layer - 1].transpose())\n",
    "        # retuning gradients for biases and weights of neurons\n",
    "        return (grad_b, grad_w)\n",
    "\n",
    "    def train_network(self, training_data, beta, evolutions=5):\n",
    "        # taking inputs and values\n",
    "        training_data_inputs = training_data[0]\n",
    "        training_data_values = training_data[1]\n",
    "        for _ in range(evolutions):\n",
    "            # taking x as vector of inputs to network and number as class\n",
    "            for x, y in zip(training_data_inputs, training_data_values):\n",
    "                # zeros at start\n",
    "                gradients_b = [np.zeros(b.shape) for b in self.biases]\n",
    "                gradients_w = [np.zeros(w.shape) for w in self.weights]\n",
    "                # instead of single value, y should be list of n elements with values 0.0 or 1.0\n",
    "                # ex: for y=7 and n_of_classes = 10, y_list = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]\n",
    "                y_list = np.zeros(shape=(self.n_of_classess,1))\n",
    "                y_list[y] = 1.0\n",
    "                # getting gradients for each layer\n",
    "                deriv_gradients_b, deriv_gradients_w = self.backward_propagation(x, y_list)\n",
    "                # summing up gradients\n",
    "                gradients_b = [grad_b + d_grad_b for grad_b, d_grad_b in zip(gradients_b, deriv_gradients_b)]\n",
    "                gradients_w = [grad_w + d_grad_w for grad_w, d_grad_w in zip(gradients_w, deriv_gradients_w)]\n",
    "                # setting up weights and biases of network\n",
    "                self.weights = [weight - beta * grad_w for weight, grad_w in zip(self.weights, gradients_w)]\n",
    "                self.biases = [bias - beta * grad_b for bias, grad_b in zip(self.biases, gradients_b)]\n",
    "\n",
    "    def score_network(self, data_set):\n",
    "        # taking inputs and outputs\n",
    "        data_inputs = data_set[0]\n",
    "        data_outputs = data_set[1]\n",
    "        # how many correct\n",
    "        result = 0\n",
    "        # for every input, output\n",
    "        for x, y in zip(data_inputs, data_outputs):\n",
    "            # classify input as creating array (10,1)\n",
    "            x_result = self.forward_propagation(x)\n",
    "            # choose best class (max of elements)\n",
    "            maxx = -1\n",
    "            ind_maxx = None\n",
    "            for ind, val in enumerate(x_result):\n",
    "                if val > maxx:\n",
    "                    maxx = val\n",
    "                    ind_maxx = ind\n",
    "            # compare to output\n",
    "            if y == ind_maxx:\n",
    "                result += 1\n",
    "        # return value is how_many_good / how_many_tested\n",
    "        return result / len(data_inputs)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z) * (1.0 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Funkcja walidacyjna\n",
    "+ Funkcja VALIDATE_NETWORK otrzymuje jako parametry: listę hiperparametrów do sprawdzenia, wartość parametru beta dla spadku gradientu, zbiór testowy i zbiór walidacyjny.\n",
    "+ Sprawdza ona jak działa dany hiperparametr poprzez wytrenowanie z jego użyciem sieci na zbiorze testowym i przetestowanie na zbiorze walidacyjnym.\n",
    "+ Funkcja zwraca sieć nauczoną na hiperparametrze, na którym sukces klasyfikacji był największy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_network(hiperparam_list, SGD_beta, train_set, valid_set):\n",
    "    best_network = None\n",
    "    best_score = 0\n",
    "    best_hiperparam = None\n",
    "    # for every hiperparam\n",
    "    for hiperparam in hiperparam_list:\n",
    "        net = Network(hiperparam)\n",
    "        net.train_network(train_set, SGD_beta)\n",
    "        net_score = net.score_network(valid_set)\n",
    "        print(f\"Score for network with hiperparam: {hiperparam} equals {net_score}\")\n",
    "        # choose if best so far\n",
    "        if net_score > best_score:\n",
    "            best_score = net_score\n",
    "            best_network = net\n",
    "            best_hiperparam = hiperparam\n",
    "    print(f\"Best network is with hiperparam: {best_hiperparam} with score {best_score}\")\n",
    "    return best_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Funkcja testująca\n",
    "+ Funkcja wyświetla sukces testu sieci na zbiorze testującym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_network(network, test_set):\n",
    "    score = network.score_network(test_set)\n",
    "    print(f\"Score for network on test set equals: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test implementacji\n",
    "### 4.1. Definicja hiperparametrów, walidacja na zbiorze uczącym i walidacyjnym\n",
    "+ Na początku występuje wydzielenie zbiorów: testowego, walidacyjnego i testowego.\n",
    "+ Następnie zostaje zdefiniowana lista hiperaparametrów do przebadania.\n",
    "+ Ustalony zostaje parametr beta dla GD.\n",
    "+ Na końcu wywoływana jest operacja walidacji sieci.\n",
    "+ Wynik: zbiór stosunków poprawnych klasyfikacji zbioru walidacyjnego do wszystkich przeprowadzonych klasyfikacji dla każdego hiperparametru (np. 0.9123 = 91,23% sukces).\n",
    "+ Dodatkowo zostaje przedstawione, który z hiperparametrów daje najlepsza klasyfikację dla zbioru walidacyjnego, wytrenowana na nim sieć jest zapisywana do zmiennej network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for network with hiperparam: [784, 30, 10] equals 0.9561\n",
      "Score for network with hiperparam: [784, 100, 80, 55, 30, 10] equals 0.9135\n",
      "Score for network with hiperparam: [784, 10, 10] equals 0.9239\n",
      "Score for network with hiperparam: [784, 800, 10] equals 0.9738\n",
      "Score for network with hiperparam: [784, 300, 300, 10] equals 0.9686\n",
      "Score for network with hiperparam: [784, 200, 100, 10] equals 0.9679\n",
      "Score for network with hiperparam: [784, 100, 10] equals 0.9704\n",
      "Score for network with hiperparam: [784, 300, 10] equals 0.975\n",
      "Score for network with hiperparam: [784, 300, 200, 100, 50, 20, 10] equals 0.103\n",
      "Score for network with hiperparam: [784, 100, 100, 100, 10] equals 0.9564\n",
      "Score for network with hiperparam: [784, 1, 10] equals 0.2061\n",
      "Best network is with hiperparam: [784, 300, 10] with score 0.975\n"
     ]
    }
   ],
   "source": [
    "TRAIN_SET, VALIDATION_SET, TEST_SET = generate_mnist_datasets()\n",
    "hiperparams_list = [\n",
    "    [784, 30, 10],\n",
    "    [784, 100, 80, 55, 30, 10],\n",
    "    [784, 10, 10],\n",
    "    [784, 800, 10],\n",
    "    [784, 300, 300, 10],\n",
    "    [784, 200, 100, 10],\n",
    "    [784, 100, 10],\n",
    "    [784, 300, 10],\n",
    "    [784, 300, 200, 100, 50, 20, 10],\n",
    "    [784, 100, 100, 100, 10],\n",
    "    [784, 1, 10]\n",
    "]\n",
    "beta = 0.1\n",
    "network = validate_network(hiperparams_list, beta, TRAIN_SET, VALIDATION_SET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Dla większości problemów jedna warstwa ukryta w zupełności wystarcza, natomiast musi występować odpowiednia liczba neuronów w tej warstwie.\n",
    "+ Użycie zbyt małej ilości neuronów w warstwie ukrytej (np. [784, 1, 10]) może doprowadzić do zjawiska zwanego underfitting. Wówczas nasz model jest zbyt prosty, by móc poradzić sobie z danym zadaniem (w przypadku zbioru MNIST model sprawdza czy zestaw pixeli dopasowuje sie do tylko jednego mozliwego ksztaltu - prawdopodobnie za każdym razem, niezależnie od danych wejsciowych wybiera jedną albo dwie liczby, które mu odpowiadają, stąd wynik to 20%, jednak nie ma w tym żadnej użyteczności). Zazwyczaj dobrym rozwiązaniem jest zwiększenie ilości neuronów w danej warstwie.\n",
    "+ Zjawiskiem przeciwnym do underfitting jest overfitting. Overfitting występuje wtedy, gdy sieć neuronowa ma tak dużą zdolność przetwarzania informacji, że ograniczona ilość informacji zawarta w zestawie treningowym nie wystarcza do wytrenowania wszystkich neuronów w warstwach ukrytych. Taka sytuacja mogła zajść w przykładzie z warstwami [784, 300, 200, 100, 50, 20, 10]. Model stał się tak skomplikowany, że dla zbioru MNIST jako daną liczbę wykrywał tylko tą samą, ale napisaną zaledwie za pomocą niewielu charakterów pisma.\n",
    "+ Oczywiście dodatkowym problemem ze zbyt dużą ilością warstw ukrytych i liczby neuronów jest czas potrzebny na wytrenowanie, który może, dla naprawdę skomplikowanych sieci, być bardzo długi.\n",
    "+ Istnieje wiele możliwości doboru odpowiednich parametrów. Dla większości sytuacji w zupełności wystarczy jedna warstwa ukryta. Większy problem jest z doborem liczby neuronów. Przeważnie liczba ta powinna być z przedziału (liczba neuronów warstwy wejściowej; liczba neuronów warstwy wyjściowej), przy czym nie powinna być ona za mała.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Ostateczny test sieci\n",
    "+ Sieć nauczona na zbiorze trenującym z hiperparametrem wybranym na podstawie zbioru walidacyjnego jest wykorzystywana do przetestowania klasyfikatora na zbiorze testującym.\n",
    "+ W wyniku otrzymujemy sukces (stosunek poprawnych klasyfikacji do wszystkich klasyfikacji)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for network on test set equals: 0.9743\n"
     ]
    }
   ],
   "source": [
    "test_network(network, TEST_SET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Jak widać, sieć popełnia błąd przy klasyfikacji 2,57% podanych próbek, co oznacza, że sieć jest bardzo dobrym klasyfikatorem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
